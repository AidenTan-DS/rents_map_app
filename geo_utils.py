import os
import zipfile
import tempfile
from pathlib import Path

import numpy as np
import pandas as pd
import geopandas as gpd
import streamlit as st

from config_data import (
    CBSA_SHP_PATH,
    ZCTA_SHP_PATH,
    MANUAL_CBSA_NAME_MAP,
)
from config_data import compute_rankings


# ============================================================
# 1. é€šç”¨ï¼šä»Žæœ¬åœ°æˆ– GitHub Release ZIP åŠ è½½ shapefile
# ============================================================

def _get_zip_url_from_secrets(key: str) -> str:
    """
    ä»Ž st.secrets æˆ–çŽ¯å¢ƒå˜é‡ä¸­æ‹¿ ZIP ä¸‹è½½é“¾æŽ¥ã€‚
    ä¾‹å¦‚åœ¨ .streamlit/secrets.toml é‡Œé…ç½®:
        CBSA_ZIP_URL = "https://github.com/.../cbsa_shapes.zip"
        ZCTA_ZIP_URL = "https://github.com/.../zcta_shapes.zip"
    """
    # st.secrets é‡Œä¼˜å…ˆ
    if key in st.secrets:
        return st.secrets[key]
    # é€€ä¸€æ­¥ç”¨çŽ¯å¢ƒå˜é‡
    return os.getenv(key, "")


def _download_and_extract_zip(zip_url: str, label: str) -> Path:
    """
    ä¸‹è½½ zip åˆ°ä¸´æ—¶ç›®å½•å¹¶è§£åŽ‹ï¼Œè¿”å›žè§£åŽ‹åŽçš„ç›®å½•è·¯å¾„ã€‚
    è¿™ä¸ªå‡½æ•°åªåœ¨ cache é‡Œè°ƒç”¨ï¼Œæ‰€ä»¥åªä¼šæ‰§è¡Œä¸€æ¬¡ã€‚
    """
    if not zip_url:
        raise RuntimeError(f"{label}: æœªé…ç½® ZIP ä¸‹è½½é“¾æŽ¥ï¼ˆåœ¨ secrets.toml é‡Œè®¾ç½® {label}_ZIP_URLï¼‰")

    # ä½¿ç”¨ Streamlit çš„ä¸´æ—¶ç›®å½•
    tmp_root = Path(tempfile.gettempdir()) / "rents_map_shapes"
    tmp_root.mkdir(parents=True, exist_ok=True)

    zip_path = tmp_root / f"{label.lower()}.zip"
    extract_dir = tmp_root / label.lower()
    extract_dir.mkdir(parents=True, exist_ok=True)

    # å¦‚æžœ zip å·²ç»å­˜åœ¨å°±ä¸å†ä¸‹è½½ï¼ˆç®€å•ä¸€ç‚¹ï¼‰
    if not zip_path.exists():
        # ä¸é¢å¤–ä¾èµ– requestsï¼Œç›´æŽ¥ç”¨ urllib
        import urllib.request

        try:
            st.write(f"â¬‡ï¸ Downloading {label} shapefile ZIP ...")
            urllib.request.urlretrieve(zip_url, zip_path.as_posix())
        except Exception as e:
            raise RuntimeError(f"{label}: ä¸‹è½½ ZIP å¤±è´¥ï¼Œè¯·æ£€æŸ¥ URL æ˜¯å¦æ­£ç¡®: {e}")

    # è§£åŽ‹ï¼ˆå¦‚æžœå·²ç»è§£åŽ‹è¿‡ï¼Œå†è§£åŽ‹ä¸€æ¬¡ä¹Ÿæ²¡å…³ç³»ï¼‰
    try:
        with zipfile.ZipFile(zip_path, "r") as zf:
            zf.extractall(extract_dir)
    except Exception as e:
        raise RuntimeError(f"{label}: è§£åŽ‹ ZIP å¤±è´¥ï¼Œè¯·ç¡®è®¤ä¸Šä¼ çš„æ–‡ä»¶æ˜¯æœ‰æ•ˆçš„ zip: {e}")

    return extract_dir


def _find_shp_file(root_dir: Path) -> Path:
    """
    åœ¨è§£åŽ‹åŽçš„ç›®å½•é‡Œé€’å½’å¯»æ‰¾ç¬¬ä¸€ä¸ª .shp æ–‡ä»¶ã€‚
    å‡è®¾æ¯ä¸ª ZIP é‡Œåªæ”¾ä¸€å¥— shapefileã€‚
    """
    shp_files = list(root_dir.rglob("*.shp"))
    if not shp_files:
        raise RuntimeError(f"åœ¨ {root_dir} ä¸‹æ‰¾ä¸åˆ°ä»»ä½• .shp æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ ZIP å†…å®¹ã€‚")
    return shp_files[0]


def _load_shapefile(local_path: str, zip_url_key: str, label: str) -> gpd.GeoDataFrame:
    """
    ä¼˜å…ˆä½¿ç”¨æœ¬åœ° shapefileï¼ˆä¾‹å¦‚ data/*.shpï¼‰ï¼Œ
    æœ¬åœ°æ²¡æœ‰æ—¶ï¼Œä»Ž GitHub Release çš„ ZIP ä¸‹è½½å¹¶åŠ è½½ã€‚
    """
    local_path_obj = Path(local_path)

    # 1) æœ¬åœ°è·¯å¾„å­˜åœ¨ï¼šç›´æŽ¥è¯»
    if local_path_obj.exists():
        return gpd.read_file(local_path_obj.as_posix())

    # 2) æœ¬åœ°ä¸å­˜åœ¨ï¼šä»Ž ZIP ä¸‹è½½
    zip_url = _get_zip_url_from_secrets(zip_url_key)
    extract_dir = _download_and_extract_zip(zip_url, label)
    shp_path = _find_shp_file(extract_dir)
    return gpd.read_file(shp_path.as_posix())


# ============================================================
# 2. åŠ è½½ ZCTA / CBSA è¾¹ç•Œ
# ============================================================

@st.cache_resource(show_spinner="ðŸ—ºï¸ Loading ZIP code boundaries...")
def load_zcta_shapes() -> gpd.GeoDataFrame:
    """
    åŠ è½½ ZIP (ZCTA) shapefileï¼š
    - æœ¬åœ°æœ‰ ZCTA_SHP_PATH å°±ç”¨æœ¬åœ°
    - å¦åˆ™ä»Ž ZCTA_ZIP_URL ä¸‹è½½ ZIPï¼Œè§£åŽ‹åŽè‡ªåŠ¨æ‰¾ .shp
    """
    gdf = _load_shapefile(ZCTA_SHP_PATH, "ZCTA_ZIP_URL", "ZCTA")

    if "ZCTA5CE10" not in gdf.columns:
        raise RuntimeError("ZCTA shapefile ç¼ºå°‘å­—æ®µ 'ZCTA5CE10'ã€‚è¯·ç¡®è®¤ç”¨çš„æ˜¯ Census ZCTA shapefileã€‚")

    gdf["zip_code_str"] = gdf["ZCTA5CE10"].astype(str)
    return gdf


@st.cache_resource(show_spinner="ðŸ™ï¸ Loading metro area boundaries...")
def load_cbsa_shapes() -> gpd.GeoDataFrame:
    """
    åŠ è½½ CBSA shapefileï¼š
    - æœ¬åœ°æœ‰ CBSA_SHP_PATH å°±ç”¨æœ¬åœ°
    - å¦åˆ™ä»Ž CBSA_ZIP_URL ä¸‹è½½ ZIPï¼Œè§£åŽ‹åŽè‡ªåŠ¨æ‰¾ .shp
    """
    gdf = _load_shapefile(CBSA_SHP_PATH, "CBSA_ZIP_URL", "CBSA")

    if "NAME" not in gdf.columns:
        raise RuntimeError("CBSA shapefile ç¼ºå°‘å­—æ®µ 'NAME'ã€‚è¯·ç¡®è®¤ç”¨çš„æ˜¯ CBSA shapefileã€‚")

    gdf["name_lower"] = gdf["NAME"].astype(str).str.lower()
    return gdf


# ============================================================
# 3. City / CBSA åŒ¹é…é€»è¾‘ï¼ˆåŸºæœ¬ä¸å˜ï¼‰
# ============================================================

def parse_city_state(city: str, city_full: str):
    raw = city_full or city or ""
    raw = str(raw)
    parts = [p.strip() for p in raw.split(",")]
    if len(parts) >= 2:
        city_part = parts[0]
        state_part = parts[1]
    else:
        city_part = parts[0] if parts else ""
        state_part = ""
    city_base = city_part.strip()
    state_abbrev = state_part.strip().upper()[:2] if state_part else ""
    return city_base, state_abbrev


def build_city_tokens(city_base: str):
    city_base = (city_base or "").strip().lower()
    if not city_base:
        return []
    tokens = [city_base]
    for sep in ["-", "â€“", "â€”"]:
        if sep in city_base:
            tokens.extend([t.strip() for t in city_base.split(sep) if t.strip()])
    return list(dict.fromkeys(tokens))


def resolve_manual_cbsa_name(city: str, city_full: str):
    key = (city_full or city or "").strip().lower()
    if key in MANUAL_CBSA_NAME_MAP:
        return MANUAL_CBSA_NAME_MAP[key]
    if "boston" in key:
        return "Boston-Cambridge-Newton, MA-NH"
    return None


@st.cache_data
def build_city_cbsa_polygons(
    df_city: pd.DataFrame,
    _cbsa_gdf: gpd.GeoDataFrame,
    metric_name: str,
) -> gpd.GeoDataFrame:
    """Match each city (metro) in df_city to a CBSA polygon."""
    cbsa_gdf = _cbsa_gdf.copy()
    if "name_lower" not in cbsa_gdf.columns:
        cbsa_gdf["name_lower"] = cbsa_gdf["NAME"].astype(str).str.lower()

    cbsa_4326 = cbsa_gdf.to_crs(epsg=4326)
    centroids = cbsa_4326.geometry.centroid
    cbsa_gdf["centroid_lat"] = centroids.y
    cbsa_gdf["centroid_lon"] = centroids.x

    cbsa_name_lower = cbsa_gdf["name_lower"]
    cbsa_name_upper = cbsa_gdf["NAME"].astype(str).str.upper()

    records = []

    for _, row in df_city.iterrows():
        city = str(row["city"])
        city_full = str(row.get("city_full", city)).strip()
        avg_value = row["avg_metric_value"]
        lat0 = float(row.get("lat", np.nan))
        lon0 = float(row.get("lon", np.nan))

        if not city_full:
            continue

        candidates = cbsa_gdf.iloc[0:0]

        manual_name = resolve_manual_cbsa_name(city, city_full)
        if manual_name:
            manual_matches = cbsa_gdf[cbsa_gdf["NAME"] == manual_name]
            if not manual_matches.empty:
                best = manual_matches.iloc[0]
                records.append(
                    {
                        "city": city,
                        "city_full": city_full,
                        "metro_name": city_full,
                        "avg_metric_value": avg_value,
                        "geometry": best.geometry,
                    }
                )
                continue

        city_full_lower = city_full.lower()
        exact = cbsa_gdf[cbsa_name_lower == city_full_lower]
        if exact.empty:
            contains = cbsa_gdf[cbsa_name_lower.str.contains(city_full_lower, na=False)]
        else:
            contains = exact
        candidates = contains

        if candidates.empty:
            city_base, state_abbrev = parse_city_state(city, city_full)
            tokens = build_city_tokens(city_base)
            if tokens:
                base_mask = cbsa_name_lower.apply(
                    lambda name: any(t in name for t in tokens)
                )
                if base_mask.any():
                    if state_abbrev:
                        state_mask = cbsa_name_upper.str.contains(state_abbrev, na=False)
                        mask = base_mask & state_mask
                        if mask.any():
                            candidates = cbsa_gdf[mask]
                    else:
                        candidates = cbsa_gdf[base_mask]

        if candidates.empty:
            continue

        if (
            len(candidates) > 1
            and np.isfinite(lat0)
            and np.isfinite(lon0)
        ):
            cand = candidates.copy()
            dlat = cand["centroid_lat"] - lat0
            dlon = cand["centroid_lon"] - lon0
            cand["dist2"] = dlat * dlat + dlon * dlon
            cand = cand.sort_values("dist2")
            best = cand.iloc[0]
        else:
            best = candidates.iloc[0]

        records.append(
            {
                "city": city,
                "city_full": city_full,
                "metro_name": city_full,
                "avg_metric_value": avg_value,
                "geometry": best.geometry,
            }
        )

    if not records:
        return gpd.GeoDataFrame(
            columns=["city", "city_full", "metro_name", "avg_metric_value", "geometry"]
        )

    gdf_out = gpd.GeoDataFrame(records, geometry="geometry", crs=cbsa_gdf.crs)
    gdf_out = compute_rankings(gdf_out, "avg_metric_value", "city")
    return gdf_out


def get_zip_polygons_for_metro(selected_city, zcta_shapes, df_zip_metric):
    """
    For a given selected_city, return:
    - zip_df_city: metric values for ZIPs in this metro
    - gdf_merge: ZCTA polygons merged with metric values
    """
    zip_df_city = (
        df_zip_metric[df_zip_metric["city"] == selected_city]
        .dropna(subset=["metric_value"])
        .reset_index(drop=True)
    )
    if zip_df_city.empty:
        return zip_df_city, gpd.GeoDataFrame()

    zip_df_small = zip_df_city[["zip_code_str", "metric_value", "city_full"]].drop_duplicates()
    gdf_merge = zcta_shapes.merge(zip_df_small, on="zip_code_str", how="inner")
    return zip_df_city, gdf_merge
