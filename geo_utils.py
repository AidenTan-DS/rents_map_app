import os
import numpy as np
import pandas as pd
import geopandas as gpd
import streamlit as st

from config_data import (
    CBSA_SHP_PATH,
    ZCTA_SHP_PATH,
    CBSA_ZIP_PATH,
    ZCTA_ZIP_PATH,
    MANUAL_CBSA_NAME_MAP,
)
from config_data import compute_rankings


# =========================
# 1. Shapefile loading
# =========================

def _resolve_shapefile_path(shp_path: str, zip_path: str, label: str) -> str:
    """
    ä¼˜å…ˆä½¿ç”¨æœªå‹ç¼© .shpï¼Œå¦‚æœæ²¡æœ‰ï¼Œå†ç”¨åŒç›®å½•ä¸‹çš„ .zipã€‚
    è¿”å›å¯ä»¥ä¼ ç»™ geopandas.read_file çš„è·¯å¾„ï¼š
      - ç›´æ¥ .shp è·¯å¾„ï¼Œæˆ–è€…
      - 'zip://data/xxx.zip'
    """
    # ä¼˜å…ˆç”¨ .shp
    if shp_path and os.path.exists(shp_path):
        return shp_path

    # å…¶æ¬¡ç”¨ zip
    if zip_path and os.path.exists(zip_path):
        # GeoPandas æ”¯æŒç›´æ¥è¯»å– 'zip://path/to/zip'
        return f"zip://{zip_path}"

    # ä¸¤ä¸ªéƒ½ä¸å­˜åœ¨ï¼ŒæŠ¥é”™
    raise RuntimeError(
        f"{label}: æ‰¾ä¸åˆ°æœ¬åœ° shapefileï¼Œ"
        f"é¢„æœŸä½ç½®ï¼š'{shp_path}' æˆ– '{zip_path}'ã€‚"
    )


@st.cache_resource(show_spinner="ğŸ—ºï¸ Loading ZIP code boundaries...")
def load_zcta_shapes() -> gpd.GeoDataFrame:
    """åŠ è½½ ZCTAï¼ˆZIP Code Tabulation Areaï¼‰è¾¹ç•Œã€‚"""
    path = _resolve_shapefile_path(ZCTA_SHP_PATH, ZCTA_ZIP_PATH, "ZCTA")
    gdf = gpd.read_file(path)

    # ç¡®è®¤åˆ—å
    if "ZCTA5CE10" not in gdf.columns:
        raise RuntimeError("ZCTA shapefile ç¼ºå°‘ 'ZCTA5CE10' åˆ—ã€‚")

    gdf["zip_code_str"] = gdf["ZCTA5CE10"].astype(str).str.zfill(5)
    return gdf


@st.cache_resource(show_spinner="ğŸ™ï¸ Loading metro area boundaries...")
def load_cbsa_shapes() -> gpd.GeoDataFrame:
    """åŠ è½½ CBSAï¼ˆå¤§éƒ½å¸‚ç»Ÿè®¡åŒºï¼‰è¾¹ç•Œã€‚"""
    path = _resolve_shapefile_path(CBSA_SHP_PATH, CBSA_ZIP_PATH, "CBSA")
    gdf = gpd.read_file(path)

    if "NAME" not in gdf.columns:
        raise RuntimeError("CBSA shapefile ç¼ºå°‘ 'NAME' åˆ—ã€‚")

    gdf["name_lower"] = gdf["NAME"].astype(str).str.lower()
    return gdf


# =========================
# 2. City / CBSA åŒ¹é…å·¥å…·
# =========================

def parse_city_state(city: str, city_full: str):
    raw = city_full or city or ""
    raw = str(raw)
    parts = [p.strip() for p in raw.split(",")]
    if len(parts) >= 2:
        city_part = parts[0]
        state_part = parts[1]
    else:
        city_part = parts[0] if parts else ""
        state_part = ""
    city_base = city_part.strip()
    state_abbrev = state_part.strip().upper()[:2] if state_part else ""
    return city_base, state_abbrev


def build_city_tokens(city_base: str):
    city_base = (city_base or "").strip().lower()
    if not city_base:
        return []
    tokens = [city_base]
    for sep in ["-", "â€“", "â€”"]:
        if sep in city_base:
            tokens.extend([t.strip() for t in city_base.split(sep) if t.strip()])
    # å»é‡ï¼Œä¿æŒé¡ºåº
    return list(dict.fromkeys(tokens))


def resolve_manual_cbsa_name(city: str, city_full: str):
    key = (city_full or city or "").strip().lower()
    if key in MANUAL_CBSA_NAME_MAP:
        return MANUAL_CBSA_NAME_MAP[key]
    # ç‰¹ä¾‹ï¼šBoston ä¸€ç±»
    if "boston" in key:
        return "Boston-Cambridge-Newton, MA-NH"
    return None


@st.cache_data
def build_city_cbsa_polygons(
    df_city: pd.DataFrame,
    _cbsa_gdf: gpd.GeoDataFrame,
    metric_name: str,
) -> gpd.GeoDataFrame:
    """
    æ ¹æ® city(city_full) æŠŠæ¯ä¸ª metro åŒ¹é…åˆ°ä¸€ä¸ª CBSA polygonã€‚
    è¾“å‡ºä¸€ä¸ª GeoDataFrameï¼Œç”¨äº metro-level Choroplethã€‚
    """
    cbsa_gdf = _cbsa_gdf.copy()
    if "name_lower" not in cbsa_gdf.columns:
        cbsa_gdf["name_lower"] = cbsa_gdf["NAME"].astype(str).str.lower()

    # é¢„å…ˆç®—å¥½ CBSA çš„è´¨å¿ƒï¼Œæ–¹ä¾¿ç”¨ (lat, lon) é€‰æœ€è¿‘çš„ä¸€ä¸ª
    cbsa_4326 = cbsa_gdf.to_crs(epsg=4326)
    centroids = cbsa_4326.geometry.centroid
    cbsa_gdf["centroid_lat"] = centroids.y
    cbsa_gdf["centroid_lon"] = centroids.x

    cbsa_name_lower = cbsa_gdf["name_lower"]
    cbsa_name_upper = cbsa_gdf["NAME"].astype(str).str.upper()

    records = []

    for _, row in df_city.iterrows():
        city = str(row["city"])
        city_full = str(row.get("city_full", city)).strip()
        avg_value = row["avg_metric_value"]
        lat0 = float(row.get("lat", np.nan))
        lon0 = float(row.get("lon", np.nan))

        if not city_full:
            continue

        candidates = cbsa_gdf.iloc[0:0]

        # 1. æ‰‹åŠ¨æ˜ å°„ï¼ˆDCã€Boston ç­‰ç‰¹æ®Šæƒ…å†µï¼‰
        manual_name = resolve_manual_cbsa_name(city, city_full)
        if manual_name:
            manual_matches = cbsa_gdf[cbsa_gdf["NAME"] == manual_name]
            if not manual_matches.empty:
                best = manual_matches.iloc[0]
                records.append(
                    {
                        "city": city,
                        "city_full": city_full,
                        "metro_name": city_full,
                        "avg_metric_value": avg_value,
                        "geometry": best.geometry,
                    }
                )
                continue

        # 2. ç›´æ¥ç”¨ city_full åšç²¾ç¡®åŒ¹é… / contains
        city_full_lower = city_full.lower()
        exact = cbsa_gdf[cbsa_name_lower == city_full_lower]
        if exact.empty:
            contains = cbsa_gdf[cbsa_name_lower.str.contains(city_full_lower, na=False)]
        else:
            contains = exact
        candidates = contains

        # 3. ç”¨ city + state token åšæ¨¡ç³ŠåŒ¹é…
        if candidates.empty:
            city_base, state_abbrev = parse_city_state(city, city_full)
            tokens = build_city_tokens(city_base)
            if tokens:
                base_mask = cbsa_name_lower.apply(
                    lambda name: any(t in name for t in tokens)
                )
                if base_mask.any():
                    if state_abbrev:
                        state_mask = cbsa_name_upper.str.contains(state_abbrev, na=False)
                        mask = base_mask & state_mask
                        if mask.any():
                            candidates = cbsa_gdf[mask]
                    else:
                        candidates = cbsa_gdf[base_mask]

        if candidates.empty:
            continue

        # å¤šä¸ªå€™é€‰æ—¶ï¼Œç”¨ (lat, lon) ç¦»å¾—æœ€è¿‘çš„
        if (
            len(candidates) > 1
            and np.isfinite(lat0)
            and np.isfinite(lon0)
        ):
            cand = candidates.copy()
            dlat = cand["centroid_lat"] - lat0
            dlon = cand["centroid_lon"] - lon0
            cand["dist2"] = dlat * dlat + dlon * dlon
            cand = cand.sort_values("dist2")
            best = cand.iloc[0]
        else:
            best = candidates.iloc[0]

        records.append(
            {
                "city": city,
                "city_full": city_full,
                "metro_name": city_full,
                "avg_metric_value": avg_value,
                "geometry": best.geometry,
            }
        )

    if not records:
        return gpd.GeoDataFrame(
            columns=["city", "city_full", "metro_name", "avg_metric_value", "geometry"]
        )

    gdf_out = gpd.GeoDataFrame(records, geometry="geometry", crs=cbsa_gdf.crs)
    gdf_out = compute_rankings(gdf_out, "avg_metric_value", "city")
    return gdf_out


# =========================
# 3. Metro â†’ ZIP polygons
# =========================

def get_zip_polygons_for_metro(selected_city, zcta_shapes, df_zip_metric):
    """
    ç»™å®š selected_cityï¼Œè¿”å›ï¼š
      - zip_df_city: è¿™ä¸ª metro é‡Œã€æ¯ä¸ª ZIP çš„ metric å€¼
      - gdf_merge: ZCTA polygon + metric merge åçš„ GeoDataFrame
    """
    zip_df_city = (
        df_zip_metric[df_zip_metric["city"] == selected_city]
        .dropna(subset=["metric_value"])
        .reset_index(drop=True)
    )
    if zip_df_city.empty:
        return zip_df_city, gpd.GeoDataFrame()

    zip_df_small = zip_df_city[["zip_code_str", "metric_value", "city_full"]].drop_duplicates()
    gdf_merge = zcta_shapes.merge(zip_df_small, on="zip_code_str", how="inner")
    return zip_df_city, gdf_merge
